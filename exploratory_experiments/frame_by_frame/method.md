 In the frame-by-frame setting, at every conversation turn, the LLM calls one of the lowest-level AAI motion commands: Forward, Backward, Right, or Left. Alongside the mandatory motion command, the LLM has the option to include a Think command. At every conversation turn, LLM-AAI responds to the LLM with an image observation, resulting from the LLM's previous motion command. In this manner, the LLM can no longer call compound commands, such as Go(30), which  large portions of the arena in one turn.
 
 The frame-by-frame paradigm allows for finer-grained control and, arguably, a more direct comparison to the RL agents and children. While covering long distances takes up more conversation turns in the frame-by-frame setting, the LLM gets more observations in the process and can rectify its intended trajectory at any turn. This increased sensitivity, in control and observation, is both useful when the agent miscalculates its trajectory and when the environment changes as the agent moves. The latter is, for example, the case in arenas with moving balls. In the frame-by-frame setting, the LLM does not miss such environment updates.

 This higher sensitivity for the LLM came with considerable increases in API cost. Indeed, we had to increase the maximum number of conversation turns for the agent to cover similar distances as in the original setting and complete the tasks. 

This cost increase and our limited budget for this exploratory study imposed the following constraints on our experiment. We only tested GPT-4o (gpt-4o-2024-05-13), over the first three levels of our test suite (12 tasks in total), with no repetitions, with a maximum of 150 conversation turns per run, and a resolution of 500 by 500 pixels for observation images. This image resolution was chosen because of OpenAI's API pricing: input images with resolution up to 500 by 500 pixels are on the lowest cost-tier.

Finally, to avoid wasting resources, we set the following early-stoppage condition: if the LLM had sent the same exact command or had been alternating between the same two commands for 50 turns or more, we halted that run. Then, that run’s final reward was set to the reward that the agent would have gotten had it kept sending the same command up until the end of the episode. The value of 50 was chosen because to get from one end of the arena to the other, in the direction of the arena’s edges takes 35 Forward commands, and in the direction of the arena’s diagonal takes aorund 43 Forward commands. Hence, any identical command repeated for more than 43 turns would not be sensible. To add a safety factor we chose to round up the stop-conditions’s threshold to 50 turns.